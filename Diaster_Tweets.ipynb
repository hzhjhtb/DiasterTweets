{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a style=\"font-size: 25pt; font-weight: bold\">Natural Language Processing with Disaster Tweets</a></center>\n",
    "<br/>\n",
    "<center><a style=\"font-size: 18pt; font-weight: bold\">Binary Classification on Tweets texts</a></center>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:magenta\">Group Names:</span>\n",
    "\n",
    "* Zhe HUANG\n",
    "* Lanshi FU\n",
    "* Pierre QIU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Description\n",
    "\n",
    "Twitter has become an important communication channel in times of emergency. The ubiquitousness of smartphones enables people to announce an emergency they’re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).  \n",
    "\n",
    "In this Kaggle competition, we aim to build a machine learning model that predicts which Tweets are about real disasters and which Tweets aren’t. We have access to a dataset of 10,000 tweets that were hand classified.\n",
    "\n",
    "This is a binary classification task on (relatively) short texts; The size of dataset is very tiny. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import model_selection, pipeline\n",
    "\n",
    "import fasttext\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from simpletransformers.classification import ClassificationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\qiu87\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\qiu87\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "StopWords = set(stopwords.words('English'))\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Have a look at the data\n",
    "\n",
    "Our data contains 4 columns, keyword, location, text and target:\n",
    "* id - a unique identifier for each tweet\n",
    "* text - the text of the tweet\n",
    "* location - the location the tweet was sent from (may be blank)\n",
    "* keyword - a particular keyword from the tweet (may be blank)\n",
    "* target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   keyword location                                               text  target\n",
       "id                                                                            \n",
       "1      NaN      NaN  Our Deeds are the Reason of this #earthquake M...       1\n",
       "4      NaN      NaN             Forest fire near La Ronge Sask. Canada       1\n",
       "5      NaN      NaN  All residents asked to 'shelter in place' are ...       1\n",
       "6      NaN      NaN  13,000 people receive #wildfires evacuation or...       1\n",
       "7      NaN      NaN  Just got sent this photo from Ruby #Alaska as ...       1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"./Data/train.csv\", index_col='id')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   keyword location                                               text\n",
       "id                                                                    \n",
       "0      NaN      NaN                 Just happened a terrible car crash\n",
       "2      NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "3      NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "9      NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"./Data/test.csv\", index_col='id')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Drop the duplicate rows in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 7613 Tweets as training data. The first thing we should do is to drop the duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 52 duplicate rows in the training set.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are\", np.sum(train.duplicated()), \"duplicate rows in the training set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7561, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.drop_duplicates()\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After dropping the duplicate rows, there remain 7561 samples.  \n",
    "This is a relatively small number, compared with millions of Tweets online. As a result, our model may not be generalizable enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Check for completeness of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 Tweets have no keywords\n",
      "2500 Tweets have no location\n",
      "0 Tweets have no text\n",
      "0 Tweets have no target\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(train['keyword'].isna()), \"Tweets have no keywords\")\n",
    "print(np.sum(train['location'].isna()), \"Tweets have no location\")\n",
    "print(np.sum(train['text'].isna()), \"Tweets have no text\")\n",
    "print(np.sum(train['text'].isna()), \"Tweets have no target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Check for balance of targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4322\n",
       "1    3239\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_train = train['target'].value_counts()\n",
    "counts_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.pie(x=counts_train, labels=counts_train.index, autopct='%.2f%%')\n",
    "plt.title(\"Comparing Tweets is a real diaster(1) or not(0)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3239(42.82%) diaster Tweets and 4322(57.16%) non-diaster Tweets.  \n",
    "Thus we have an about 3:4 class balance of targets. This is not perfect but still can be used for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    This function is to clean the Tweet texts.\n",
    "\n",
    "    :param text: Tweet text\n",
    "    :return temp: cleaned Tweet text\n",
    "    \"\"\"\n",
    "    temp = text.lower() # make text lowercase\n",
    "    temp = re.sub('\\n', ' ' , temp) #remove \\n\n",
    "    temp = re.sub('\\'', '', temp) #remove \\'\n",
    "    temp = re.sub('-', ' ', temp) #remove -\n",
    "    temp = re.sub(r'(http|https|pic.)\\S', ' ', temp) #remove links\n",
    "    temp = re.sub(r'[^\\w\\s]', ' ', temp) #remove symbols\n",
    "    \n",
    "    return temp\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"\n",
    "    This function is to remove stopwords. Stopwords are those those words unuseless like 'they', 'am', 'been', 'about'.\n",
    "    \n",
    "    :param text: Tweet text with stopwords\n",
    "    :return temp: Tweet text without stopwords\n",
    "    \"\"\"\n",
    "    tokenized_words = word_tokenize(text) # word tokenization\n",
    "    temp = [word for word in tokenized_words if word not in StopWords] # remain those words who are not stopwords\n",
    "    temp = ' '.join(temp) # convert tokenized to text\n",
    "    \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By cleaning the data, the remained texts become more informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deeds reason earthquake may allah forgive us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>residents asked shelter place notified officer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13 000 people receive wildfires evacuation ord...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfires pou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   keyword location                                               text  \\\n",
       "id                                                                       \n",
       "1      NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "4      NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "5      NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "6      NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "7      NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "    target                                              clean  \n",
       "id                                                             \n",
       "1        1       deeds reason earthquake may allah forgive us  \n",
       "4        1              forest fire near la ronge sask canada  \n",
       "5        1  residents asked shelter place notified officer...  \n",
       "6        1  13 000 people receive wildfires evacuation ord...  \n",
       "7        1  got sent photo ruby alaska smoke wildfires pou...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['clean'] = train['text'].apply(clean_text)\n",
    "train['clean'] = train['clean'].apply(remove_stopwords)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>happened terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>heard earthquake different cities stay safe ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>forest fire spot pond geese fleeing across str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>apocalypse lighting spokane wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>typhoon soudelor kills 28 china taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   keyword location                                               text  \\\n",
       "id                                                                       \n",
       "0      NaN      NaN                 Just happened a terrible car crash   \n",
       "2      NaN      NaN  Heard about #earthquake is different cities, s...   \n",
       "3      NaN      NaN  there is a forest fire at spot pond, geese are...   \n",
       "9      NaN      NaN           Apocalypse lighting. #Spokane #wildfires   \n",
       "11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan   \n",
       "\n",
       "                                                clean  \n",
       "id                                                     \n",
       "0                         happened terrible car crash  \n",
       "2   heard earthquake different cities stay safe ev...  \n",
       "3   forest fire spot pond geese fleeing across str...  \n",
       "9               apocalypse lighting spokane wildfires  \n",
       "11             typhoon soudelor kills 28 china taiwan  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['clean'] = test['text'].apply(clean_text)\n",
    "test['clean'] = test['clean'].apply(remove_stopwords)\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Tokenization(with Lemmatization and Stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_normalization(tokens):\n",
    "    \"\"\"\n",
    "    This function is to convert tokens into its base form. First Lemmatization and then Stemming.\n",
    "\n",
    "    :param tokens: token got by nltk.tokenize.word_tokenize\n",
    "    :parma temp: tokens/words in original form\n",
    "    \"\"\"\n",
    "    # Lemmatizer: mainly cats -> cat\n",
    "    lemmatizer=nltk.stem.WordNetLemmatizer()\n",
    "    temp = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Stemmer: mainly talked -> talk\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    temp = [stemmer.stem(token) for token in temp]\n",
    "\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deeds reason earthquake may allah forgive us</td>\n",
       "      <td>[deed, reason, earthquak, may, allah, forgiv, u]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>[forest, fire, near, la, rong, sask, canada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>residents asked shelter place notified officer...</td>\n",
       "      <td>[resid, ask, shelter, place, notifi, offic, ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13 000 people receive wildfires evacuation ord...</td>\n",
       "      <td>[13, 000, peopl, receiv, wildfir, evacu, order...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfires pou...</td>\n",
       "      <td>[got, sent, photo, rubi, alaska, smoke, wildfi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   keyword location                                               text  \\\n",
       "id                                                                       \n",
       "1      NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "4      NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "5      NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "6      NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "7      NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "    target                                              clean  \\\n",
       "id                                                              \n",
       "1        1       deeds reason earthquake may allah forgive us   \n",
       "4        1              forest fire near la ronge sask canada   \n",
       "5        1  residents asked shelter place notified officer...   \n",
       "6        1  13 000 people receive wildfires evacuation ord...   \n",
       "7        1  got sent photo ruby alaska smoke wildfires pou...   \n",
       "\n",
       "                                               tokens  \n",
       "id                                                     \n",
       "1    [deed, reason, earthquak, may, allah, forgiv, u]  \n",
       "4        [forest, fire, near, la, rong, sask, canada]  \n",
       "5   [resid, ask, shelter, place, notifi, offic, ev...  \n",
       "6   [13, 000, peopl, receiv, wildfir, evacu, order...  \n",
       "7   [got, sent, photo, rubi, alaska, smoke, wildfi...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['tokens'] = train['clean'].apply(word_tokenize)\n",
    "train['tokens'] = train['tokens'].apply(token_normalization)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>clean</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>happened terrible car crash</td>\n",
       "      <td>[happen, terribl, car, crash]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>heard earthquake different cities stay safe ev...</td>\n",
       "      <td>[heard, earthquak, differ, citi, stay, safe, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>forest fire spot pond geese fleeing across str...</td>\n",
       "      <td>[forest, fire, spot, pond, goos, flee, across,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>apocalypse lighting spokane wildfires</td>\n",
       "      <td>[apocalyps, light, spokan, wildfir]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>typhoon soudelor kills 28 china taiwan</td>\n",
       "      <td>[typhoon, soudelor, kill, 28, china, taiwan]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   keyword location                                               text  \\\n",
       "id                                                                       \n",
       "0      NaN      NaN                 Just happened a terrible car crash   \n",
       "2      NaN      NaN  Heard about #earthquake is different cities, s...   \n",
       "3      NaN      NaN  there is a forest fire at spot pond, geese are...   \n",
       "9      NaN      NaN           Apocalypse lighting. #Spokane #wildfires   \n",
       "11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan   \n",
       "\n",
       "                                                clean  \\\n",
       "id                                                      \n",
       "0                         happened terrible car crash   \n",
       "2   heard earthquake different cities stay safe ev...   \n",
       "3   forest fire spot pond geese fleeing across str...   \n",
       "9               apocalypse lighting spokane wildfires   \n",
       "11             typhoon soudelor kills 28 china taiwan   \n",
       "\n",
       "                                               tokens  \n",
       "id                                                     \n",
       "0                       [happen, terribl, car, crash]  \n",
       "2   [heard, earthquak, differ, citi, stay, safe, e...  \n",
       "3   [forest, fire, spot, pond, goos, flee, across,...  \n",
       "9                 [apocalyps, light, spokan, wildfir]  \n",
       "11       [typhoon, soudelor, kill, 28, china, taiwan]  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['tokens'] = test['clean'].apply(word_tokenize)\n",
    "test['tokens'] = test['tokens'].apply(token_normalization)\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Last Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now remove those unused information in the two dataframes, only remain id, tokens (and targets in training set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(columns=['keyword', 'location', 'clean', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.drop(columns=['keyword', 'location', 'clean', 'text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do vectorization, we convert tokens into texts again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qiu87\\AppData\\Local\\Temp\\ipykernel_13868\\2213565989.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train['texts'][i] = \" \".join(token for token in train['tokens'][i])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>tokens</th>\n",
       "      <th>texts</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[deed, reason, earthquak, may, allah, forgiv, u]</td>\n",
       "      <td>deed reason earthquak may allah forgiv u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[forest, fire, near, la, rong, sask, canada]</td>\n",
       "      <td>forest fire near la rong sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>[resid, ask, shelter, place, notifi, offic, ev...</td>\n",
       "      <td>resid ask shelter place notifi offic evacu she...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>[13, 000, peopl, receiv, wildfir, evacu, order...</td>\n",
       "      <td>13 000 peopl receiv wildfir evacu order califo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>[got, sent, photo, rubi, alaska, smoke, wildfi...</td>\n",
       "      <td>got sent photo rubi alaska smoke wildfir pour ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    target                                             tokens  \\\n",
       "id                                                              \n",
       "1        1   [deed, reason, earthquak, may, allah, forgiv, u]   \n",
       "4        1       [forest, fire, near, la, rong, sask, canada]   \n",
       "5        1  [resid, ask, shelter, place, notifi, offic, ev...   \n",
       "6        1  [13, 000, peopl, receiv, wildfir, evacu, order...   \n",
       "7        1  [got, sent, photo, rubi, alaska, smoke, wildfi...   \n",
       "\n",
       "                                                texts  \n",
       "id                                                     \n",
       "1            deed reason earthquak may allah forgiv u  \n",
       "4                forest fire near la rong sask canada  \n",
       "5   resid ask shelter place notifi offic evacu she...  \n",
       "6   13 000 peopl receiv wildfir evacu order califo...  \n",
       "7   got sent photo rubi alaska smoke wildfir pour ...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['texts'] = train['tokens']\n",
    "for i in train.index:\n",
    "    train['texts'][i] = \" \".join(token for token in train['tokens'][i])\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>texts</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[happen, terribl, car, crash]</td>\n",
       "      <td>happen terribl car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[heard, earthquak, differ, citi, stay, safe, e...</td>\n",
       "      <td>heard earthquak differ citi stay safe everyon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[forest, fire, spot, pond, goos, flee, across,...</td>\n",
       "      <td>forest fire spot pond goos flee across street ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[apocalyps, light, spokan, wildfir]</td>\n",
       "      <td>apocalyps light spokan wildfir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[typhoon, soudelor, kill, 28, china, taiwan]</td>\n",
       "      <td>typhoon soudelor kill 28 china taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tokens  \\\n",
       "id                                                      \n",
       "0                       [happen, terribl, car, crash]   \n",
       "2   [heard, earthquak, differ, citi, stay, safe, e...   \n",
       "3   [forest, fire, spot, pond, goos, flee, across,...   \n",
       "9                 [apocalyps, light, spokan, wildfir]   \n",
       "11       [typhoon, soudelor, kill, 28, china, taiwan]   \n",
       "\n",
       "                                                texts  \n",
       "id                                                     \n",
       "0                            happen terribl car crash  \n",
       "2       heard earthquak differ citi stay safe everyon  \n",
       "3   forest fire spot pond goos flee across street ...  \n",
       "9                      apocalyps light spokan wildfir  \n",
       "11              typhoon soudelor kill 28 china taiwan  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['texts'] = test['tokens']\n",
    "for i in test.index:\n",
    "    test['texts'][i] = \" \".join(token for token in test['tokens'][i])\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We split the train set to get a validation set\n",
    "train_xs, valid_xs, train_ys, valid_ys = model_selection.train_test_split(\n",
    "    train[\"texts\"], train[\"target\"], test_size=0.3, random_state=0, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Baseline approach I with Word2vec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Find the best model  \n",
    "\n",
    "We do grid search in order to find the best vectorizer and the best classification model along with their best hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Grid Search Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search set for hyperparameters of SVC\n",
    "\n",
    "ngram_range=[(1, 1),(1,2),(1,3)]\n",
    "max_features =  [500,1000,1500,2000,5000,10000]\n",
    "stop_words =  [None,'english']\n",
    "\n",
    "param_grid_SVC = {\n",
    "              \"vectorizer__max_features\" : max_features,\n",
    "              \"vectorizer__stop_words\" : stop_words,\n",
    "              \"vectorizer__ngram_range\" : ngram_range,\n",
    "              \"model__tol\" : [1e-8,1e-7,1e-6,1e-5,1e-4,1e-3],\n",
    "              \"model__C\" : [0.001,0.1,1,2],\n",
    "              \"model__max_iter\" : [10000]\n",
    "            }\n",
    "\n",
    "# grid search set for hyperparameters of RandomForest\n",
    "\n",
    "n_estimators =  np.arange(1, 101, 20) \n",
    "max_depth =  np.arange(1, 101, 20)\n",
    "min_samples_split = [2,4,8,16]\n",
    "min_samples_leaf = [1,2,4,8]\n",
    "\n",
    "param_grid_RF = {\n",
    "              \"vectorizer__max_features\" : max_features,\n",
    "              \"vectorizer__stop_words\" : stop_words,\n",
    "              \"vectorizer__ngram_range\" : ngram_range,\n",
    "              \"model__n_estimators\":n_estimators,\n",
    "              \"model__max_depth\":max_depth,\n",
    "              \"model__min_samples_split\":min_samples_split,\n",
    "              \"model__min_samples_leaf\":min_samples_leaf\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe_Tfidf_SVC = pipeline.Pipeline( steps= [ ('vectorizer', TfidfVectorizer()) , ('model', LinearSVC()) ] )\n",
    "pipe_Count_SVC = pipeline.Pipeline( steps= [('vectorizer', CountVectorizer()),('model', LinearSVC())])\n",
    "pipe_Tfidf_RF = pipeline.Pipeline( steps= [('vectorizer', TfidfVectorizer()),('model', RandomForestClassifier())])\n",
    "pipe_Count_RF = pipeline.Pipeline(  steps= [('vectorizer', CountVectorizer()),('model', RandomForestClassifier())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Grid Search Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 864 candidates, totalling 4320 fits\n",
      "Pipeline(steps=[('vectorizer',\n",
      "                 TfidfVectorizer(max_features=5000, ngram_range=(1, 2))),\n",
      "                ('model', LinearSVC(C=0.1, max_iter=10000, tol=1e-08))])\n",
      "best score:\n",
      "0.8002668637352711\n",
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Tfidf Vectorizer and SVM model \n",
    "\n",
    "Tfidf_SVC = model_selection.GridSearchCV(\n",
    "    pipe_Tfidf_SVC, param_grid_SVC, n_jobs=-1, verbose=3, cv=5\n",
    ")\n",
    "Tfidf_SVC.fit(train_xs, train_ys)\n",
    "\n",
    "print(Tfidf_SVC.best_estimator_)\n",
    "print('best score:')\n",
    "print(Tfidf_SVC.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 864 candidates, totalling 4320 fits\n",
      "Pipeline(steps=[('vectorizer',\n",
      "                 CountVectorizer(max_features=10000, ngram_range=(1, 2))),\n",
      "                ('model', LinearSVC(C=0.1, max_iter=10000, tol=1e-08))])\n",
      "best score:\n",
      "0.7927064980873278\n",
      "Wall time: 1min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# CounterVectorizer and SVM model\n",
    "\n",
    "Count_SVC = model_selection.GridSearchCV(\n",
    "    pipe_Count_SVC, param_grid_SVC, n_jobs=-1, verbose=3, cv=5\n",
    ")\n",
    "Count_SVC.fit(train_xs, train_ys)\n",
    "\n",
    "print(Count_SVC.best_estimator_)\n",
    "print('best score:')\n",
    "print(Count_SVC.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 14400 candidates, totalling 72000 fits\n",
      "Pipeline(steps=[('vectorizer',\n",
      "                 CountVectorizer(max_features=1500, ngram_range=(1, 3))),\n",
      "                ('model',\n",
      "                 RandomForestClassifier(max_depth=81, min_samples_split=16,\n",
      "                                        n_estimators=81))])\n",
      "best score:\n",
      "0.7876053843998065\n",
      "Wall time: 32min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# CounterVectorizer and RandomForest model\n",
    "\n",
    "Count_RF = model_selection.GridSearchCV(\n",
    "    pipe_Count_RF, param_grid_RF, n_jobs=-1, verbose=3, cv=5\n",
    ")\n",
    "Count_RF.fit(train_xs, train_ys)\n",
    "\n",
    "print(Count_RF.best_estimator_)\n",
    "print('best score:')\n",
    "print(Count_RF.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 14400 candidates, totalling 72000 fits\n",
      "Pipeline(steps=[('vectorizer', TfidfVectorizer(max_features=1500)),\n",
      "                ('model',\n",
      "                 RandomForestClassifier(max_depth=81, min_samples_leaf=2,\n",
      "                                        min_samples_split=4,\n",
      "                                        n_estimators=61))])\n",
      "best score:\n",
      "0.7864709903946906\n",
      "Wall time: 34min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Tfidf Vectorizer and RandomForest model\n",
    "\n",
    "Tfidf_RF = model_selection.GridSearchCV(\n",
    "    pipe_Tfidf_RF, param_grid_RF, n_jobs=-1, verbose=3, cv=5\n",
    ")\n",
    "Tfidf_RF.fit(train_xs, train_ys)\n",
    "\n",
    "print(Tfidf_RF.best_estimator_)\n",
    "print('best score:')\n",
    "print(Tfidf_RF.best_score_)\n",
    "predictions = Tfidf_RF.best_estimator_.predict(valid_xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Test with the best hyperparameters\n",
    "\n",
    "We found that the best combination of vectorizer and classification model is Tfidf_SVC!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = Tfidf_SVC.best_estimator_.predict(test[\"texts\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       1\n",
       "1   2       1\n",
       "2   3       1\n",
       "3   9       1\n",
       "4  11       1\n",
       "5  12       1\n",
       "6  21       0\n",
       "7  22       0\n",
       "8  27       0\n",
       "9  29       0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating submission data.\n",
    "\n",
    "submission = pd.read_csv('./Data/sample_submission.csv')\n",
    "submission['target'] = predictions\n",
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving submission to '.csv' file:\n",
    "\n",
    "submission.to_csv('./Data/submission_Word2vec.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We uploaded **submission_Word2vec.csv** into kaggle and got a score of **0.78424**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Baseline approach II with Fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Generate text file for Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate train set for fasttext\n",
    "with open(\"./Data/train_fasttext.txt\", 'w',encoding='utf-8') as f:\n",
    "    for i in train.index:\n",
    "        line = \"__label__%d %s\\n\" % (train[\"target\"][i], train[\"texts\"][i])\n",
    "        f.write(line)\n",
    "    f.close()\n",
    "\n",
    "# generate test set for fasttext\n",
    "with open(\"./Data/test_fasttext.txt\", 'w',encoding='utf-8') as f:\n",
    "    for i in test.index:\n",
    "        line = \"%s\\n\" % test[\"texts\"][i]\n",
    "        f.write(line)\n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first several lines of train_fasttext.txt:\n",
      "\n",
      "__label__1 deed reason earthquak may allah forgiv u\n",
      "__label__1 forest fire near la rong sask canada\n",
      "__label__1 resid ask shelter place notifi offic evacu shelter place order expect\n",
      "__label__1 13 000 peopl receiv wildfir evacu order california\n",
      "__label__1 got sent photo rubi alaska smoke wildfir pour school\n",
      "\n",
      "The first several lines of test_fasttext.txt:\n",
      "\n",
      "happen terribl car crash\n",
      "heard earthquak differ citi stay safe everyon\n",
      "forest fire spot pond goos flee across street save\n",
      "apocalyps light spokan wildfir\n",
      "typhoon soudelor kill 28 china taiwan\n"
     ]
    }
   ],
   "source": [
    "print(\"The first several lines of train_fasttext.txt:\\n\")\n",
    "with open(\"./Data/train_fasttext.txt\", 'r',encoding='utf-8') as f:\n",
    "    for i in range(5):\n",
    "        print(f.readline(), end='')\n",
    "    f.close()\n",
    "\n",
    "print('\\nThe first several lines of test_fasttext.txt:\\n')\n",
    "with open(\"./Data/test_fasttext.txt\", 'r',encoding='utf-8') as f:\n",
    "    for i in range(5):\n",
    "        print(f.readline(), end='')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>targets</th>\n",
       "      <th>texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>__label__1</td>\n",
       "      <td>deed reason earthquak may allah forgiv u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>__label__1</td>\n",
       "      <td>forest fire near la rong sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>__label__1</td>\n",
       "      <td>resid ask shelter place notifi offic evacu she...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>__label__1</td>\n",
       "      <td>13 000 peopl receiv wildfir evacu order califo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>__label__1</td>\n",
       "      <td>got sent photo rubi alaska smoke wildfir pour ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7556</th>\n",
       "      <td>__label__1</td>\n",
       "      <td>two giant crane hold bridg collaps nearbi home...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7557</th>\n",
       "      <td>__label__1</td>\n",
       "      <td>aria_ahrari thetawniest control wild fire cali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7558</th>\n",
       "      <td>__label__1</td>\n",
       "      <td>m1 94 01 04 utc 5km volcano hawaii co zdtoyd8ebj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7559</th>\n",
       "      <td>__label__1</td>\n",
       "      <td>polic investig e bike collid car littl portug ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7560</th>\n",
       "      <td>__label__1</td>\n",
       "      <td>latest home raze northern california wildfir a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7561 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         targets                                              texts\n",
       "0     __label__1           deed reason earthquak may allah forgiv u\n",
       "1     __label__1               forest fire near la rong sask canada\n",
       "2     __label__1  resid ask shelter place notifi offic evacu she...\n",
       "3     __label__1  13 000 peopl receiv wildfir evacu order califo...\n",
       "4     __label__1  got sent photo rubi alaska smoke wildfir pour ...\n",
       "...          ...                                                ...\n",
       "7556  __label__1  two giant crane hold bridg collaps nearbi home...\n",
       "7557  __label__1  aria_ahrari thetawniest control wild fire cali...\n",
       "7558  __label__1   m1 94 01 04 utc 5km volcano hawaii co zdtoyd8ebj\n",
       "7559  __label__1  polic investig e bike collid car littl portug ...\n",
       "7560  __label__1  latest home raze northern california wildfir a...\n",
       "\n",
       "[7561 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the fasttext file.\n",
    "with open(\"./Data/train_fasttext.txt\", 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "\n",
    "train_xs= []\n",
    "train_ys = []\n",
    "for line in lines:\n",
    "    label, text = line.strip().split(' ', 1)\n",
    "    train_ys.append(label)\n",
    "    train_xs.append(text)\n",
    "\n",
    "# Here we want a dataframe for grid search below\n",
    "df = pd.DataFrame(columns=['targets','texts'])\n",
    "df[\"targets\"] = train_ys\n",
    "df[\"texts\"] = train_xs\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# range of hyperparameters\n",
    "lrs =  [0.1, 0.5, 1.0, 2.0]\n",
    "epochs = [5, 10, 15, 20 ,25]\n",
    "wordNgrams = [1, 2, 3, 5]\n",
    "kf = model_selection.KFold(n_splits=5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search in order to find the best hyperparameters of fasttext.train_supervised()\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "i = 0\n",
    "estimators = pd.DataFrame(columns=['score','param'])\n",
    "for lr in lrs:\n",
    "    for epoch in epochs:\n",
    "        for wordNgram in wordNgrams:\n",
    "            #print(f\"lr:{lr},epoch:{epoch},wordNgrams:{wordNgram}\")\n",
    "            metric_score = 0.0\n",
    "            for train_idx, val_idx in kf.split(df):\n",
    "                df_train = df.loc[train_idx]\n",
    "                df_val = df.loc[val_idx]\n",
    "                tmpdir = tempfile.mkdtemp()\n",
    "                train_csv = tmpdir + '/cv_train_fasttext.txt'\n",
    "                df_train.to_csv(train_csv, sep='\\t', index=False, header=None, encoding='UTF-8')\n",
    "                fast_model = fasttext.train_supervised(train_csv, epoch=epoch, lr=lr, wordNgrams=wordNgram, verbose=2, minCount=1)\n",
    "                predictions = []\n",
    "                for line in df_val[\"texts\"]:\n",
    "                    line = line.strip()\n",
    "                    predictions.append(fast_model.predict(line)[0][0])\n",
    "                predictions= np.array(predictions)\n",
    "                score = len(predictions[predictions==df_val[\"targets\"]])/len(predictions)\n",
    "                metric_score += score\n",
    "                shutil.rmtree(tmpdir, ignore_errors=True) \n",
    "            #print('Score:', metric_score / kf.n_splits)\n",
    "            param = {}\n",
    "            param[\"epoch\"] = epoch\n",
    "            param[\"wordNgrams\"] = wordNgram\n",
    "            param[\"lr\"] = lr\n",
    "            row={'score':metric_score / kf.n_splits,'param':param}\n",
    "            estimators.loc[i] = row\n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score 0.7645799892990903 witn [{'epoch': 25, 'wordNgrams': 3, 'lr': 0.5}]\n"
     ]
    }
   ],
   "source": [
    "print(f'Best score {estimators[\"score\"].max()} witn {estimators[estimators[\"score\"]==estimators[\"score\"].max()][\"param\"].values}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Train with the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_supervised(\n",
    "        input=\"./Data/train_fasttext.txt\", epoch=15, lr=0.1, wordNgrams=3\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = []\n",
    "with open(\"./Data/test_fasttext.txt\", 'r',encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.strip()\n",
    "        predictions.append(int(model.predict(line)[0][0][-1]))\n",
    "\n",
    "predictions[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       1\n",
       "1   2       1\n",
       "2   3       1\n",
       "3   9       1\n",
       "4  11       1\n",
       "5  12       1\n",
       "6  21       0\n",
       "7  22       0\n",
       "8  27       0\n",
       "9  29       0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating submission data.\n",
    "\n",
    "submission = pd.read_csv('./Data/sample_submission.csv')\n",
    "submission['target'] = predictions\n",
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving submission to '.csv' file:\n",
    "\n",
    "submission.to_csv('./Data/submission_fasttext.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We uploaded **submission_fasttext.csv** into kaggle and got a score of **0.80202**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Advanced approach with Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qiu87\\AppData\\Local\\Temp\\ipykernel_13868\\300826876.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"pred_class\"] = train[\"target\"]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>pred_class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>deed reason earthquak may allah forgiv u</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>forest fire near la rong sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>resid ask shelter place notifi offic evacu she...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13 000 peopl receiv wildfir evacu order califo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>got sent photo rubi alaska smoke wildfir pour ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                texts  pred_class\n",
       "id                                                               \n",
       "1            deed reason earthquak may allah forgiv u           1\n",
       "4                forest fire near la rong sask canada           1\n",
       "5   resid ask shelter place notifi offic evacu she...           1\n",
       "6   13 000 peopl receiv wildfir evacu order califo...           1\n",
       "7   got sent photo rubi alaska smoke wildfir pour ...           1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = train[[\"texts\"]]\n",
    "df[\"pred_class\"] = train[\"target\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Build Bert Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a model from a pre-trained base(distilbert-base-uncased model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing BertForSequenceClassification: ['distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'vocab_projector.weight', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'vocab_transform.weight', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'vocab_transform.bias', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'vocab_projector.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['encoder.layer.2.attention.self.value.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.7.attention.self.query.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.6.attention.self.query.weight', 'classifier.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.8.attention.self.query.weight', 'classifier.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.weight', 'pooler.dense.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.7.intermediate.dense.weight', 'embeddings.LayerNorm.weight', 'pooler.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.4.attention.self.value.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.11.output.LayerNorm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "# define hyperparameter\n",
    "train_args ={\"reprocess_input_data\": False,\n",
    "             \"fp16\":False,\n",
    "             \"num_train_epochs\": 2}\n",
    "\n",
    "# Create a ClassificationModel\n",
    "model = ClassificationModel(\n",
    "    \"bert\", \"distilbert-base-uncased\",\n",
    "    num_labels=2,\n",
    "    args=train_args,\n",
    "    use_cuda=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Train & Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py:612: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74a474f1beef4b08b545dbb184d09726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d294f65b65044c5b47a277edea82361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 2:   0%|          | 0/946 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cba607a0f284674832441c6871eaf78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 2:   0%|          | 0/946 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1892, 0.54629970622499)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train_model(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a0e9de9e85c465e838e67cdbcc9cde3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3263 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3b88bc6f0e04202a1c378cc10e14ea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = model.predict(test[\"texts\"].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       1\n",
       "1   2       0\n",
       "2   3       1\n",
       "3   9       0\n",
       "4  11       1\n",
       "5  12       0\n",
       "6  21       0\n",
       "7  22       0\n",
       "8  27       0\n",
       "9  29       0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating submission data.\n",
    "\n",
    "submission = pd.read_csv('./Data/sample_submission.csv')\n",
    "submission['target'] = predictions[0]\n",
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving submission to '.csv' file:\n",
    "\n",
    "submission.to_csv('./Data/submission_bert.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We uploaded **submission_bert.csv** into kaggle and got a score of **0.77934**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 What we succeed\n",
    "* we first anylyse the trainning data and testing data, make sure that this dataset can be used to our classification task.\n",
    "* we did the preprocessing of data which includes dropping the duplication of samples, cleaning the tweets texts, Lemmatization and Stemming,etc\n",
    "* we used 3 different methodes to classifier the tweets: traditionel Word2vec, Fasttext and advanced bert whih is based on transformer\n",
    "* we also used grid search to find the best hyperparameters for Word2vec method and Fasttext method\n",
    "* we also uploaded several times our test result into "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 What we failed\n",
    "* We actually tried to create our own bert model with the pretrained huggingface model, which means we have to fine tune the first layer of the model. Since this is a little hard for our current level, we gave it up.\n",
    "* We also tried to do grid search for the current used bert model but that failed too due to the time limit.\n",
    "* At the very beginning, we tried different combinations of Lemmatization and Stemming; But it turns out that first Lemmatization and then Stemming is the best choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 If we had more time\n",
    "* We would establish our own burt model, but before that, we had to learn bert model step by step\n",
    "* It's also worthy to see whether the result will become better if we delete several steps of preprocessing(as we know, the training and grid search takes time)\n",
    "* Actually at the end we found that this dataset is too small and maybe we can try with a bigger tweets dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "78363424069b5231720f289dab8ee51c1c7b1689bb8cdb34f26f6c0007fb72f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
